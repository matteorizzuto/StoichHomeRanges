---
title: "Snowshoe hare home range size: estimation via Kernel Utilization Distribution"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
author: "Matteo Rizzuto"
output:
  rmdformats::html_clean:
    self_contained: false
    thumbnails: true
    lightbox: true
    gallery: true
    highlight: tango
    code_folding: hide
    fig_width: 8
    fig_height: 6
    df_print: paged
  pdf_document:
    citation_package: biblatex
    dev: pdf
    fig_caption: true
    fig_crop: TRUE
    highlight: tango
    keep_tex: true
    latex_engine: pdflatex
    toc: true
    toc_depth: 2
    number_sections: false
    df_print: kable    
fontsize: 11pt
geometry: margin=1in
documentclass: article    
bibliography: ../Manuscripts/Rizzuto_etal_StoichHomeRanges.bib
csl: ../Manuscripts/ecology.csl
editor_options: 
  chunk_output_type: inline
---

## Data Loading and Cleaning
Over the course of four years, from 2016 to 2019, our research group 
live-trapped, collared, and followed 38 snowshoe hares over four sampling grids. 
These are, from younger (20--40 years old) to oldest (81--100 years old): 
Bloomfield (n=35), Unicorn (n=1), Terra Nova North (n=1), and Dunphy's Pond 
(n=1). As the bulk of individuals followed was found in the Bloomfield grid, we 
will focus on this population of hares. Each year, live trapping occurred in the 
spring and fall seasons, whereas triangulation took place during the summer 
months (i.e., May-September). Triangulation started in 2017. See the main text 
and Supplementary Information for details on animal handling and live trapping 
protocols and permits. 

<!-- Throughout each data collection season, some collared snowshoe hares died. In -->
<!-- some cases, this happened before we had a chance to collect enough relocations -->
<!-- for producing a reliable estimate of these individuals' home ranges. Because of -->
<!-- this, we removed these individuals from analyses (n=5). -->

Here, we begin by loading the datasets containing the triangulation data, one 
per year, and the dataset containing the live-trapping and demographic data.
```{r data-loading, error=TRUE,tidy=TRUE}
# Load the triangulation raw data
rawData17 <- read.csv("../Data/VHF_CleanData_2017.csv", header = TRUE) # 2017
rawData18 <- read.csv("../Data/VHF_CleanData_2018.csv", header = TRUE) # 2018
rawData19 <- read.csv("../Data/TelemetryPoints_VHF_2019.csv", 
                      header = TRUE) # 2019

# Switch back to VHF_CleanData_2019.csv to run with data pre Bella's corrections 
# from Dec 2019

# Load hare trapping and demographic data, this contains the eartag number
# which allows us to track if individuals survived multiple years
hareDemData <- read.csv("../Data/CollaredHareData.csv", stringsAsFactors = TRUE)

# make sure Collar Frequency and SamplingYear are factors
hareDemData$CollarFrequency <- as.factor(hareDemData$CollarFrequency)
hareDemData$SamplingYear <- as.factor(hareDemData$SamplingYear)


# load the spatial layout of our sampling grids
grid_points_all <- read_sf(dsn = "../Data/GridPoints", 
                           layer = "GridPts_withFRI")

# subset the Bloomfield grid, which we will use as a visual reference
# for the position of our grid in space
bl_grid_points <- subset(grid_points_all, 
                         grid_points_all$SiteName == "Bloomfield")

# Load the StDMs raster layers
acruCN <- raster("../Data/StDMs_Rasters/Ratios/ACRU_CN.tif")
acruNP <- raster("../Data/StDMs_Rasters/Ratios/ACRU_NP.tif")
vaanCN <- raster("../Data/StDMs_Rasters/Ratios/VAAN_CN.tif")
vaanCP <- raster("../Data/StDMs_Rasters/Ratios/VAAN_CP.tif")
vaanNP <- raster("../Data/StDMs_Rasters/Ratios/VAAN_NP.tif")

# clip rasters to study area
e <- extent(860000, 863000, 5383000, 5386000)
vaancnclip <- crop(vaanCN, e)
image(vaancnclip)
vaancpclip <- crop(vaanCP, e)
vaannpclip <- crop(vaanNP, e)
acrucnclip <- crop(acruCN, e)
acrunpclip <- crop(acruNP, e)

# stack the rasters
stoich_stack <- raster::stack(vaancnclip, vaancpclip, vaannpclip, acrucnclip, acrunpclip)

# read the forest shapefile from the Land Cover geodatabase
forest_gdb <- read_sf("../../Mapping", layer = "Forest")
```

### Caveats 
The 2019 triangulation data were collected using decimal coordinates, whereas 
for the 2017 and 2018 triangulation data we used UTM coordinates. The following 
code chunk (source: https://stackoverflow.com/a/18641016) takes care of this 
discrepancy by converting decimal Latitude and Longitude to UTM format. 
The UTM zone number for our study site is 22.

```{r UTM-conversion, echo=TRUE, tidy=TRUE}
# # assign UTM-format names to spatial coordinates in rawData19
# coordinates(rawData19) <- c("Easting", "Northing")
# # set projection for rawData19
# proj4string(rawData19) <- CRS("+proj=longlat +datum=WGS84")
# 
# # transform rawData19 to a SpatialPointsDataFrame, this is the step that 
# # performs the actual conversion from decimal Latitude and Longitude coordinates
# # to UTM coordinates
# rawData19 <- spTransform(rawData19, CRS("+proj=utm +zone=22 ellps=WGS84"))
# 
# # convert rawData19 back to a normal dataframe
# rawData19 <- as.data.frame(rawData19)
# # set UTM zone
# rawData19$UTMZone <- as.integer(22)
```

```{r rawdata-crs-conversion,echo=TRUE,tidy=TRUE}
# 2019 uses lat/long and 2017-2018 use UTM
# convert all of them to meters to match stoich raster CRS
# code developed by Isabella C. Richmond (https://gitlab.com/icrichmond/)
# note that ICR uses the notation "VHFXXXX" to name the telemetry datasets, 
# where XXXX is the year of sampling in its 4 digits notation 
 
rawData17 <- drop_na(rawData17, Easting)
coordinates(rawData17) <- c("Easting", "Northing")
proj4string(rawData17) <- CRS("+init=epsg:32622")

rawData17 <- spTransform(rawData17, CRS("+proj=tmerc +lat_0=0 +lon_0=-61.5 +k=0.9999 +x_0=304800 +y_0=0 +ellps=GRS80 +units=m +no_defs"))
rawData17 <- as.data.frame(rawData17)
rawData17$UTMZone <- as.integer(22)


coordinates(rawData18) <- c("Easting", "Northing")
proj4string(rawData18) <- CRS("+init=epsg:32622")

rawData18 <- spTransform(rawData18, CRS("+proj=tmerc +lat_0=0 +lon_0=-61.5 +k=0.9999 +x_0=304800 +y_0=0 +ellps=GRS80 +units=m +no_defs"))
rawData18 <- as.data.frame(rawData18)
rawData18$UTMZone <- as.integer(22)


coordinates(rawData19) <- c("Easting", "Northing")
proj4string(rawData19) <- CRS("+proj=longlat +datum=WGS84")

rawData19 <- spTransform(rawData19, CRS("+proj=tmerc +lat_0=0 +lon_0=-61.5 +k=0.9999 +x_0=304800 +y_0=0 +ellps=GRS80 +units=m +no_defs"))
rawData19 <- as.data.frame(rawData19)
rawData19$UTMZone <- as.integer(22)
```


We will not be using several variables we collected in the field in the following 
analyses. These include:

* **Notes**, which contained general comments and notes about relocations collected in the field
* **SampleTimeCat**, **SampleTimeGeneral**, and **TimeCategory**, which we created to categorize
the time of triangulation: in the final dataset, variable AM_PM has taken over their function
* **Clouds** and **Temperature** (Temp, Temp_C), were collected in different ways between 2017--2018 and 2019, due to differences in the sampling equipment, and hence they cannot be used for analyses that take into account all three years
* **Line**, which identifies the lines drawn on the iPad along a bearing collected during triangulation, provide information which is redundant with the Point variable
* **Fix.Location**, which is redundant with the UTM coordinates of the listening points and
is used for convenience in the field
* **Time_O**, which is redundant as no script uses the original time in HH:MM:SS but all
use time in HHMMSS stored in variable Time
* **Alive**, which was collected to keep track of the live collars on the grid during data
collection but is never used in the scripts

Finally, in summer 2017, the main observer (MR) had health issues that prevented
him from performing data collection for a period of three days. During this 
time, a second observer (TH) performed triangulations, to check whether the 
collared animals were alive. As these relocations covered a limited period of 
time, and for lack of an assessment of the intra-observer variability in 
triangulation skills, we exclude these relocations from all analyses.

## Telemetry Error Analyses
In 2017, a single observer performed all triangulations on the four 
radio-collared snowshoe hares active on our trapping grid. In 2018 and 2019,due 
to an increase in both the number of collared hares active on the grid and in 
the workforce available, we had three observer simultaneously collecting 
azimuths on each collar. This approach allowed for a faster completion time for
each triangulation bout and also for an improve accuracy. 

Here, we report the data and code used to estimate the accuracy of our lone
observer in 2017 (MR) and of one of the observers in 2018 (Mr. Benjamin Stratton).
During the lead-up to the 2017 and 2018 telemetry sampling seasons, we conducted 
6 days of error reduction exercises with MR in 2017, and 7 days of error 
reduction exercises with BS in 2018. During each error reduction session, the
observer would attempt to locate a variable number of collars (range: 1-7) 
hidden in advance by a collaborator. In 2017, these exercises took place on the
trapping grid itself, alongside other sampling activities. In 2018, exercises 
took place in Pippy Park, a large natural area in St. John's, NL, whose
vegetation and topography closely resemble the grid itself.

```{r error-reduction-triang, echo=TRUE, tidy=TRUE}
# load the error trials data
trialData_2017 <- read.csv("../Data/MR_ErrorReduction.csv")
trialData_2018 <- read.csv("../Data/BS_ErrorReduction.csv")

# check structure pre-merge
# head(trialData_2017)
# head(trialData_2018)

# remove unnecessary columns
trialData_2018 <- trialData_2018[, -1]
# head(trialData_2018)

# and remove NAs
trialData_2018 <- na.omit(trialData_2018)

# check a join could work
dplyr::anti_join(trialData_2017, trialData_2018)

# joint he two datasets and remove unused levels
trialData <- full_join(trialData_2017, trialData_2018) 
trialData <- droplevels(trialData)

# break down "Date" column to its individual components but keep it in the dataset
# then create a new column to uniquely identify collars between years
trialData <- trialData %>% 
  separate(., "Date", c("Year", "Month", "Day"), sep = "-", remove = FALSE) %>% 
  unite(., ID, c("Frequency", "Year"), sep = "_", remove = FALSE)  

# isolate the IDs of each data entry, i.e. its unique Frequency
IDs <- trialData$ID 

# extract unique ID values, this vector will be used multiple time to index 
# loops
UniqIDs <- unique(IDs)

# create column names for new df
OutputFields <- c(paste(colnames(trialData)), "GID") 

# add GID column to original data
trialData$GID <- NA    
trialData$GID <- as.factor(trialData$GID)

# create new df, name stands for "error reduction Data"
erData <- trialData[FALSE,] 

# remove useless GID column from original dataset
trialData$GID <- NULL 

# The following loop scans the data for groups of azimuths 
# obtained for each collar on each day of triangulation, and assigns a GID 
# based on the date of collection.

for (i in 1:length(UniqIDs)){
  # first, create a temporary dataset to host each Frequency's data in turn
  TmpInds <- which(IDs == UniqIDs[i]) 
  TmpData <- trialData[TmpInds,]   
  # assign a starting GID
  TmpData[1, "GID"] <- 1 
  for (j in 1:(nrow(TmpData) - 1)){
    # if the Date does not change, we want the GID to stay the same
      if (TmpData[j + 1, "Date"] == TmpData[j, "Date"]){
        TmpData[j + 1, "GID"] <- TmpData[j, "GID"]
      }
      else {
        # if the Date changes, we want the GID to increase accordingly
        TmpData[j + 1, "GID"] <- TmpData[j, "GID"] + 1
      }
  }
  # browser()
  # bind each temp dataset into a new one
  erData <- rbind(erData, TmpData) 
}

erData$GID <- as.factor(erData$GID)
erData$Time <- as.numeric(erData$Time)

# allocate an empty list to store each hare's triangulation results
trials.list <- vector("list", length(UniqIDs))

# run the triangulation loop
for (i in 1:length(UniqIDs)) {
  # we want to keep track of what R is doing, so let's have it print a message
  # telling us exactly which collar it is processing
  print(paste("Starting to triangulate collar", UniqIDs[i]))
  
  # Following a suggestion from sigloc's author, Simon Berg, we run the locate()
  # and plot() functions one GID at the time, for each collar.The for loop below 
  # takes care of this.
  
  # create a working dataset containing only data for the current collar
  current.triang <- subset(erData, erData$ID == UniqIDs[i])
  
  # allocate vector along whose length to run the nested loop
  current.triang.index <- unique(current.triang$GID) 
  
  # allocate an empty dataframe to store the results of the triangulation
  workingCollar.loc <- data.frame("X" = numeric(), "Y" = numeric(), 
                                  "Badpoint" = integer(), "Var_X" = numeric(), 
                                  "Var_Y" = numeric(), "Cov_XY" = numeric(), 
                                  "AngleDiff" = numeric(), "Date" = character(), 
                                  "Time" = integer(), stringsAsFactors = FALSE)
  
  # set up plotting
  # frame()
  par(mar = c(2, 2, 2, 2), mfrow = c(2,3))
  
  for (j in 1:length(current.triang.index)) {
    # separate a day's set of azimuths from the rest of the current dataset
    current.triang.dat <- subset(current.triang, current.triang$GID == j)
    
    # Prevents the looped code from looking for earlier GIDs
    current.triang.dat$GID <- 1 
    
    # save the triangulation date as a vector for later use
    date <- as.character(unique(current.triang.dat$Date))
    
    # save the collar frequency as a vector for later use
    frequency <- as.character(unique(current.triang.dat$Frequency))
    
    trlcs <- as.character(unique(current.triang.dat$TrueLoc))
    
    obs <- as.character(unique(current.triang.dat$Observer))
    
    # set the current test dataframe as a receiver for triangulation
    current.triang.rec <- as.receiver(current.triang.dat)
    
    # find the intersection points of the three azimuths in the current test df
    current.triang.int <- findintersects(current.triang.rec)
    
    xrange <- max(current.triang$Easting) - min(current.triang$Easting) # range of x values
    yrange <- max(current.triang$Northing) - min(current.triang$Northing) # range of y values
    
    xlimlow <- min(current.triang$Easting) - (0.05*xrange) # xmin - left
    xlimhigh <- max(current.triang$Easting) + (0.05*xrange) # xmax - right
    ylimlow <- min(current.triang$Northing) - (0.05*xrange) # ymin - bottom
    ylimhigh <- max(current.triang$Northing) + (0.05*xrange) # ymax - top
    
    # visualize triangulation
    plot.receiver(current.triang.rec, bearings = TRUE,
         xlab = "Easting",
         ylab = "Northing",
         ylim = c(ylimlow, ylimhigh),
         xlim = c(xlimlow, xlimhigh)
         )
    text(current.triang.rec$Easting, current.triang.rec$Northing, labels = current.triang.rec$Point, pos = 2, offset = 0.25)
    title(main = paste(unique(current.triang.dat$Frequency), unique(current.triang.rec$Date), sep = " "))
    
    # locate the transmitting collar using a Maximum Likelihood Estimator
    current.triang.loc <- locate(current.triang.rec)
    
    # add the date to the localized collar
    current.triang.loc$Date <- as.character(date) 
    
    # add information on whether true location coordinates are available 
    current.triang.loc$TrueLoc <- as.factor(trlcs)
    
    # add information on observer
    current.triang.loc$Observer <- as.factor(obs) 
    
    # plot the localized collar on the same plot as above;
    # if the point is red, it means the MLE failed to retun a point and the 
    # triangulation was complete by taking the midpoint of the azimuths' 
    # intersections to estimate the location of the collar
    
    plot.transmitter(current.triang.loc, add = TRUE, badcolor = TRUE, errors = TRUE)
    
    # save subsequent triangulations in the current workingCollar.loc dataframe 
    workingCollar.loc <- rbind(workingCollar.loc, current.triang.loc)
    
    # make sure the date is store as a character rather than a number
    workingCollar.loc$Date <- as.character(workingCollar.loc$Date)
    
    # make sure the collar frequency is stored as a character rather than a number
    # workingCollar.loc$Frequency <- as.character(workingCollar.loc$Frequency)
    
    # browser()
    # Sys.sleep(1)
  }
  
  # add Frequency identifier to the current workingCollar.loc dataframe
  workingCollar.loc$Frequency <- frequency
  # browser()
  
  # Let's take a look at these points! In order to plot them as spatial points, 
  # we first need to convert them to a Spatial Object. To do so, first we create
  # a vector containing just the lat and long coordinates of the traingulated 
  # points
  
  workingCollar.coords <- workingCollar.loc[,c(1,2)]
  
  # then we remove the corresponding columns from the dataframe
  
  workingCollar.loc[,c(1,2)] <- NULL
  
  # finally, using package sp, we combine the two to obtain a spatial object
  
  workingCollar.spatial <- SpatialPointsDataFrame(coords = workingCollar.coords, 
                                                  data = workingCollar.loc, 
                                                  proj4string = 
                                                    CRS("+proj=tmerc +lat_0=0 +lon_0=-61.5 +k=0.9999 +x_0=304800 +y_0=0 +ellps=GRS80 +units=m +no_defs"))
  
  # plotting
  # frame()
  par(mfrow=c(1,1))
  # very simple plot, just to check everything works
  plot(workingCollar.spatial, main = unique(workingCollar.spatial$Frequency))
  # add identifiers for each relocation
  text(workingCollar.spatial@coords, labels = workingCollar.spatial$Date,
       pos = 2, cex = 0.7)
  
  # store the newly produced set of triangulation for collar i into the 
  # previously allocated hares.list
  trials.list[[i]] <- workingCollar.spatial
  
  if (is.na(UniqIDs[i+1])==TRUE) {
    print(paste("Finished triangulating collar", UniqIDs[i], ";", "Done"))
  } else {print(paste("Finished triangulating collar", UniqIDs[i],";", 
                      "moving on to collar", UniqIDs[i+1]))
  }
  
  # browser()
}

# convert the list of triangulated collars dataframes into a single dataframe 
# for further analyses
trials.triangd <- do.call("rbind", trials.list)

# convert it to a sf object to use st_distance to calculate distance between trials and true locs
trials.triangd_sp <- st_as_sf(trials.triangd) %>% arrange(., Date, Frequency)

# remove trials for which we are missing true locations
trials.triangd_sp <- subset(trials.triangd_sp, trials.triangd_sp$TrueLoc != "No")
```

Now that all error trials have been triangulated, we can estimate the distance
between a collar's true location and the location estimated from MR's or BS's
triangulation efforts. We use function `st_distance` from package `sf` to measure 
the distance then, for each observer, we extract mean, median, minimum, and
maximum error, as well as the standard deviation.

```{r error-reduction-dist, echo=TRUE, tidy=TRUE}
# load dataset with true locations of test collars
trueLocs_raw <- read.csv("../Data/ErrorTrials_TrueLocs.csv")

trueLocs_raw$TrueLoc <- as.factor(trueLocs_raw$TrueLoc)

# turn it into a spatial object
trueLocs <- SpatialPointsDataFrame(coords = trueLocs_raw[, c("Easting", "Northing")], data = trueLocs_raw[, c(1:2, 6:7)], proj4string = CRS("+proj=tmerc +lat_0=0 +lon_0=-61.5 +k=0.9999 +x_0=304800 +y_0=0 +ellps=GRS80 +units=m +no_defs"))

# turn it into a sf object to use st_distance to calculate distance between tests 
# and true locs
trueLocs_sp <- st_as_sf(trueLocs) %>% rename(., Frequency = Collar) %>% arrange(., Date, Frequency)

trueLocs_sp <- subset(trueLocs_sp, trueLocs_sp$TrueLoc != "No")

# now, measure the distance in meters, between trials and true locations
trials.triangd_sp$Error <- as.numeric(st_distance(trials.triangd_sp, trueLocs_sp, by_element = TRUE))

# and get a mean value
err_mean <- with(trials.triangd_sp, tapply(trials.triangd_sp$Error, trials.triangd_sp$Observer, mean))
err_sd <- with(trials.triangd_sp, tapply(trials.triangd_sp$Error, trials.triangd_sp$Observer, sd))
err_min <- with(trials.triangd_sp, tapply(trials.triangd_sp$Error, trials.triangd_sp$Observer, min))
err_max <- with(trials.triangd_sp, tapply(trials.triangd_sp$Error, trials.triangd_sp$Observer, max))
err_median <- with(trials.triangd_sp, tapply(trials.triangd_sp$Error, trials.triangd_sp$Observer, median))


# remove temporary object that will be used later on

# rm(list = c("IDs", "UniqIDs", "TmpInds", "TmpData", "current.triang", 
#             "current.triang.dat", "current.triang.index", "current.triang.int", 
#             "current.triang.loc", "workingCollar.coords", "workingCollar.loc", 
#             "workingCollar.spatial", "date"))
```

Thus, according to our analyses, the mean error for MR in 2017 was 
**`r prettyNum(err_mean[2], digits = 3)` $\pm$ `r prettyNum(err_sd[2], digits = 3)`** m, whereas for BS in 2018 it was 
**`r prettyNum(err_mean[1], digits = 3)` $\pm$ `r prettyNum(err_sd[1], digits = 3)`** m.

## Snowshoe hare dataset preparation
After cleaning the three triangulation data sets following the caveats listed 
above, we merge them into one, called `liveData`. 

```{r data-cleanup, echo=TRUE, tidy=TRUE, message=FALSE}
# wonder if there is a way to make this more efficient, i.e. isolate 
# the live collars in a single step rather than with three repeated 
# ones?


# Isolate collars to make working on them easier
Data17 <- subset(rawData17, rawData17$Frequency == "149.535" | 
                   rawData17$Frequency == "149.673" | 
                   rawData17$Frequency == "149.394" | 
                   rawData17$Frequency == "149.452")

# Isolate collars to make working on them easier
Data18 <- subset(rawData18, rawData18$Frequency == "149.003" | 
                   rawData18$Frequency == "149.053" | 
                   rawData18$Frequency == "149.093" | 
                   rawData18$Frequency == "149.173" | 
                   rawData18$Frequency == "149.213" | 
                   rawData18$Frequency == "149.274" | 
                   rawData18$Frequency == "149.374" | 
                   rawData18$Frequency == "149.474" | 
                   rawData18$Frequency == "149.613" | 
                   rawData18$Frequency == "149.633" | 
                   rawData18$Frequency == "149.653")

# Drop data from 18-06-2018 for collar 149.653 
Data18 <- Data18[!(Data18$Frequency == "149.653" & 
                     Data18$Date == "2018-06-18"),]

# Isolate collars to make working on them easier
Data19 <- subset(rawData19, rawData19$Frequency == "149.124" | 
                   rawData19$Frequency == "149.233" | 
                   rawData19$Frequency == "149.294" | 
                   rawData19$Frequency == "149.423" | 
                   rawData19$Frequency == "149.513" | 
                   rawData19$Frequency == "149.555" | 
                   rawData19$Frequency == "149.594" | 
                   rawData19$Frequency == "150.032" | 
                   rawData19$Frequency == "150.052" | 
                   rawData19$Frequency == "150.072" | 
                   rawData19$Frequency == "150.091" | 
                   rawData19$Frequency == "150.111" | 
                   rawData19$Frequency == "150.132" | 
                   rawData19$Frequency == "150.154" | 
                   rawData19$Frequency == "150.173" | 
                   rawData19$Frequency == "150.191" | 
                   rawData19$Frequency == "150.232" | 
                   rawData19$Frequency == "150.273" | 
                   rawData19$Frequency == "150.314" | 
                   rawData19$Frequency == "150.332" | 
                   rawData19$Frequency == "150.373" | 
                   rawData19$Frequency == "150.392") 

# remove NAs
Data17 <- drop_na(Data17, Azimuth)
Data18 <- drop_na(Data18, Azimuth)
Data19 <- drop_na(Data19, Azimuth)

# Set year variable
Data17 <- add_column(Data17, Year = "2017")

Data18 <- add_column(Data18, Year = "2018")

Data19 <- add_column(Data19, Year = "2019")

# Categorize times as AM or PM in Data19
for (i in 1:(nrow(Data19) - 1)) {
   Data19$AM_PM <- ifelse(Data19$TimeCategory == "Morning","AM", "PM")
}

# Remove unused columns from the three dataframes
Data17 <- dplyr::select(Data17, -c(Notes, SampleTimeCat, SampleTimeGeneral, 
                                   Clouds, Temp, Line, Alive, Time_O))
Data18 <- dplyr::select(Data18, -c(Notes, SampleTimeCat, SampleTimeGeneral, 
                                   Clouds, Temp, Line, Alive, Time_O))
Data19 <- dplyr::select(Data19, -c(FixLocation, Temp_C, TimeCategory, Time_O))

# combine the three dataframes into a single one

liveData <- rbind(Data17, Data18, Data19) # if running analyses including 2017 data
# liveData <- rbind(Data18, Data19) # if running analyses without 2017 data

# str(liveData)

liveData$Frequency <- as.factor(liveData$Frequency)
liveData$Year <- as.factor(liveData$Year)

# remove NAs
liveData <- drop_na(liveData, Azimuth)

# remove second observer, who only did 1 day of triangulations 
# and no error reduction trials
liveData <- subset(liveData, liveData$Observer != "TH")

# use this code when knitting to html
kable(liveData, "html") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                fixed_thead = TRUE) %>%
  scroll_box(width = "100%", height = "500px")

# use this code when knitting to pdf
# kable(head(liveData), "latex", booktabs = TRUE) %>% 
#   kable_styling(latex_options = "scale_down", position = "center")
```

## Triangulation
R can perform triangulation through the package `sigloc` [@Berg2015]. This 
package uses the function `locate()` to estimate the position of a transmitter, 
in this case a VHF collar, based on three or more azimuths collected in the 
field. The algorithm used by `locate()` is based on a Maximum Likelihood 
Estimator (MLE; see package documentation and @Berg2015). In cases in which the 
MLE fails to estimate the location of the transmitter, this is estimated by 
taking the midpoint of the intersections of the lines drawn through the azimuths. 

The package `sigloc` requires all of the following elements associated with 
each azimuth:

* a date and time of collection, the latter in `HHMMSS` format
* a set of coordinates locating the listening points used to collect the azimuths
* a unique group identifier (GID) for sets of azimuths that go together, i.e. 
a collar's daily set of 3 or more azimuths

The following code assigns a unique GID to each unique combination of `Frequency` 
and `Date`. We save the new dataset in an object named `vhfData`.
```{r GID-assignment, echo=TRUE, tidy=TRUE, warning=FALSE}
# remove unused levels from factor elements within liveData
liveData <- droplevels(liveData)

# isolate the IDs of each data entry, i.e. its unique Frequency
IDs <- paste(liveData[, "Frequency"]) 

# extract unique ID values, this vector will be used multiple time to index 
# loops
UniqIDs <- unique(IDs)

# create column names for new df
OutputFields <- c(paste(colnames(liveData)), "GID") 

# add GID column to original data
liveData$GID <- NA    
liveData$GID <- as.factor(liveData$GID)

# create new df
vhfData <- liveData[FALSE,] 

# remove useless GID column from original dataset
liveData$GID <- NULL 

# The following loop scans the data for groups of azimuths 
# obtained for each collar on each day of triangulation, and assigns a GID 
# based on the date of collection.

for (i in 1:length(UniqIDs)){
  # first, create a temporary dataset to host each Frequency's data in turn
  TmpInds <- which(IDs == UniqIDs[i]) 
  TmpData <- liveData[TmpInds,]   
  # assign a starting GID
  TmpData[1, "GID"] <- 1 
  for (j in 1:(nrow(TmpData) - 1)){
    if (TmpData$Year[j] == "2017" | TmpData$Grid[j] == "UNI"){
      # if the Date stays the same, we want to assign the same GID 
      if (TmpData[j + 1, "Date"] == TmpData[j, "Date"] &&
          TmpData[j + 1, "AM_PM"] == TmpData[j, "AM_PM"]){
        TmpData[j + 1, "GID"] <- TmpData[j, "GID"]
      }
      else {
        # if the Date changes, we want the GID to increase accordingly
        TmpData[j + 1, "GID"] <- TmpData[j, "GID"] + 1
      }
    }
    else {
      if (TmpData[j + 1, "Date"] == TmpData[j, "Date"]){
        TmpData[j + 1, "GID"] <- TmpData[j, "GID"]
      }
      else {
        # if the Date changes, we want the GID to increase accordingly
        TmpData[j + 1, "GID"] <- TmpData[j, "GID"] + 1
      }
    }
  }
  # browser()
  # bind each temp dataset into a new one
  vhfData <- rbind(vhfData, TmpData) 
}

vhfData$GID <- as.factor(vhfData$GID)
vhfData$Time <- as.numeric(vhfData$Time)

kable(vhfData, "html") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                fixed_thead = TRUE) %>%
  scroll_box(width = "100%", height = "500px")

# kable(head(vhfData), "latex", booktabs = TRUE) %>% 
#   kable_styling(latex_options = "scale_down", position = "center")
```

Now that the data is ready, the code chunk below performs the triangulation. We 
use a nested `for` loop with two levels. In the "outer" loop, we use the collar
frequencies to identify and separate one collar (individual) at the time from the 
`vhfData` data set. Then we use the GID that we assigned earlier to perform the 
actual triangulation one sampling day at the time for each individual 
sequentially. We then store relocations estimated in this way in a temporary 
object, which we then bind to the final dataset to use in subsequent 
analyses, `hares.triangd`. The author of the `sigloc` package, Dr. S. Berg, 
provided the loop, which circumvents a problem arising from having 
multiple individuals in a data set --- which necessarily means the whole data set 
has repeated GIDs. The code below generates two plots for each individual:

* the first plot is a collection of smaller graphs visualizing the triangulation
of each point: here, a black dot indicates that the maximum likelihood 
estimation was successful, whereas a red dot shows that the midpoint of the
intersections was used
* the second plot shows all the relocations in space, with the date printed over
each point and the individual's collar frequency printed at the top of the page

Note that we changed azimuths = 90$^\circ$ to 91$^\circ$ degrees to avoid 
having a mathematical angle of 0, which would cause the Maximum Likelihood 
Estimator used by `locate()` to return an error. This code chunk prints a 
message to show progress in the triangulation process: this is turned off by 
default, but it can be turned on by uncommenting the relevant lines in the code 
chunk (ll. 535--540).

```{r triangulations, echo=TRUE, tidy=TRUE, warning=FALSE, message=FALSE}
# allocate an empty list to store each hare's triangulation results
hares.list <- vector("list", length(UniqIDs))

# run the triangulation loop
for (i in 1:length(UniqIDs)) {
  # we want to keep track of what R is doing, so let's have it print a message
  # telling us exactly which collar it is processing
  print(paste("Starting to triangulate collar", UniqIDs[i]))
  
  # Following a suggestion from sigloc's author, Simon Berg, we run the locate()
  # and plot() functions one GID at the time, for each collar.The for loop below 
  # takes care of this.
  
  # create a working dataset containing only data for the current collar
  current.triang <- subset(vhfData, vhfData$Frequency == UniqIDs[i])
  
  # allocate vector along whose length to run the nested loop
  current.triang.index <- unique(current.triang$GID) 
  
  # allocate an empty dataframe to store the results of the triangulation
  workingCollar.loc <- data.frame("X" = numeric(), "Y" = numeric(), 
                                  "Badpoint" = integer(), "Var_X" = numeric(), 
                                  "Var_Y" = numeric(), "Cov_XY" = numeric(), 
                                  "AngleDiff" = numeric(), "Date" = character(), 
                                  "Time" = integer(), stringsAsFactors = FALSE)

  # set up plotting
  # frame()
  par(mar = c(2, 2, 2, 2), mfrow = c(6,6))
  
  for (j in 1:length(current.triang.index)) {
    # separate a day's set of azimuths from the rest of the current dataset
    current.triang.dat <- subset(current.triang, current.triang$GID == j)
   
    # Prevents the looped code from looking for earlier GIDs
    current.triang.dat$GID <- 1 
    
    # save the triangulation date as a vector for later use
    date <- as.character(unique(current.triang.dat$Date))
    
    # set the current test dataframe as a receiver for triangulation
    current.triang.rec <- as.receiver(current.triang.dat)
    
    # find the intersection points of the three azimuths in the current test df
    current.triang.int <- findintersects(current.triang.rec)
    
    # browser()
    
    yrange <- max(current.triang.dat$Northing) - min(current.triang.dat$Northing) 
    xrange <- max(current.triang.dat$Easting) - min(current.triang.dat$Easting)
    
    ymin <- min(current.triang.dat$Northing) - 0.25*yrange
    ymax <- max(current.triang.dat$Northing) + 0.25*yrange
    xmin <- min(current.triang.dat$Easting) - 0.25*xrange
    xmax <- max(current.triang.dat$Easting) + 0.25*xrange
    
    # visualize triangulation
    plot.receiver(current.triang.rec, bearings = TRUE,
         xlab = "Easting",
         ylab = "Northing", asp = 1,
         ylim = c(ymin, ymax),
         xlim = c(xmin, xmax))
    title(main = paste(unique(current.triang.dat$Frequency), unique(current.triang.rec$Date)))
    
    # locate the transmitting collar using a Maximum Likelihood Estimator
    current.triang.loc <- locate(current.triang.rec)
    
    # add the date to the localized collar
    current.triang.loc$Date <- as.character(date) 
    
    # plot the localized collar on the same plot as above;
    # if the point is red, it means the MLE failed to retun a point and the 
    # triangulation was complete by taking the midpoint of the azimuths' 
    # intersections to estimate the location of the collar
    
    plot.transmitter(current.triang.loc, add = TRUE, badcolor = TRUE, errors = TRUE)
    
    # save subsequent triangulations in the current workingCollar.loc dataframe 
    workingCollar.loc <- rbind(workingCollar.loc, current.triang.loc)
    
    # make sure the date is store as a character rather than a number
    workingCollar.loc$Date <- as.character(workingCollar.loc$Date)
    # browser()
    # Sys.sleep(1)
  }
  
  # add Frequency identifier to the current workingCollar.loc dataframe
  workingCollar.loc$Frequency <- as.character(UniqIDs[i])
  workingCollar.loc$EarTag <- as.character(unique(current.triang$EarTag))
  # browser()
  
  # Let's take a look at these points! In order to plot them as spatial points, 
  # we first need to convert them to a Spatial Object. To do so, first we create
  # a vector containing just the lat and long coordinates of the traingulated 
  # points
  
  workingCollar.coords <- workingCollar.loc[,c(1,2)]
  
  # then we remove the corresponding columns from the dataframe
  
  workingCollar.loc[,c(1,2)] <- NULL
  
  # finally, using package sp, we combine the two to obtain a spatial object
  
  workingCollar.spatial <- SpatialPointsDataFrame(coords = workingCollar.coords, 
                                                  data = workingCollar.loc, 
                                                  proj4string = 
                                                    CRS("+proj=tmerc +lat_0=0 +lon_0=-61.5 +k=0.9999 +x_0=304800 +y_0=0 +ellps=GRS80 +units=m +no_defs"))
  
  # plotting
  frame()
  par(mfrow=c(1,1))
  # very simple plot, just to check everything works
  plot(workingCollar.spatial, main = unique(workingCollar.spatial$Frequency))
  # add identifiers for each relocation
  text(workingCollar.spatial@coords, labels = workingCollar.spatial$Date,
       pos = 2, cex = 0.7)
  
  # store the newly produced set of triangulation for collar i into the 
  # previously allocated hares.list
  hares.list[[i]] <- workingCollar.spatial
  
  # provide a message on the status of the triangulation process
  if (is.na(UniqIDs[i+1])==TRUE) {
    print(paste("Finished triangulating collar", UniqIDs[i], ";", "Done"))
  } else {print(paste("Finished triangulating collar", UniqIDs[i],";", 
                      "moving on to collar", UniqIDs[i+1]))
  }
  
 # browser()
}

# convert the list of triangulated collars dataframes into a single dataframe 
# for further analyses
hares.triangd <- do.call("rbind", hares.list)

# hares.triangd <- spTransform(hares.triangd, CRS(projargs = "+proj=tmerc +lat_0=0 +lon_0=-61.5 +k=0.9999 +x_0=304800 +y_0=0 +ellps=GRS80 +units=m +no_defs"))
```

Despite the observers' best efforts, there were cases in which the triangulation 
of an individual did not produce an accurate estimate of its location. These 
relocations may cause problems during the estimation of home range size, as they 
may stretch the home range size much as outlying points stretch a frequency 
distribution. The code chunk below highlights the individuals and the dates for 
which this happened and removes them from the data set.
```{r loc-clean-up, echo=TRUE, tidy=TRUE}
# remove triangulations from 13, 20, 26, 28 June, 29 July, 29, 30 August 2019 
# for 149.233 - FIXED AS OF DEC 20, 2019
# toBeRemoved <- which(hares.triangd$Frequency =="149.233" & 
# hares.triangd$Date == "2019-06-13" | hares.triangd$Date == "2019-06-26" | 
# hares.triangd$Date == "2019-06-28" | hares.triangd$Date == "2019-07-29" | 
# hares.triangd$Date == "2019-08-29" | hares.triangd$Date == "2019-08-30" | 
# hares.triangd$Date == "2019-06-20")
# 
# hares.triangd <- hares.triangd[-toBeRemoved, ]

# remove triangulations from 28 May, 20, 24, 27 June 2019 for 149.294  - FIXED AS OF DEC 20, 2019
# toBeRemoved <- which(hares.triangd$Frequency == "149.294" & 
# hares.triangd$Date == "2019-05-28" | hares.triangd$Date == "2019-06-24" | 
# hares.triangd$Date == "2019-06-27")
# # | hares.triangd$Date == "2019-06-20"
# hares.triangd <- hares.triangd[-toBeRemoved, ]

# remove triangulations from 16, 18, 27 May, 18, 20 June 2019 for 149.423  - FIXED AS OF DEC 20, 2019
# toBeRemoved <- which(hares.triangd$Frequency == "149.423" & 
# hares.triangd$Date == "2019-05-16")
# # | hares.triangd$Date == "2019-05-18" | hares.triangd$Date == "2019-05-27" | 
# hares.triangd$Date == "2019-06-18" | hares.triangd$Date == "2019-06-20")
# 
# hares.triangd <- hares.triangd[-toBeRemoved, ]

# remove triangulations from 25 May 2019 for 149.513  - FIXED AS OF DEC 20, 2019
# toBeRemoved <- which (hares.triangd$Frequency == "149.513" & 
# hares.triangd$Date == "2019-05-25")


# remove triangulations from 19, 22, 28 May, 18, 22, 25, 26 June 2019 
# for 150.173
toBeRemoved <- which(hares.triangd$Frequency == "150.173" & 
                       hares.triangd$Date == "2019-05-22" | 
                       hares.triangd$Date == "2019-05-28") 
# | hares.triangd$Date == "2019-06-18" | hares.triangd$Date == "2019-05-19" | 
# hares.triangd$Date == "2019-06-22"  hares.triangd$Date == "2019-06-25" | 
# hares.triangd$Date == "2019-06-26")
hares.triangd <- hares.triangd[-toBeRemoved, ]

# visualize the dataframe
kable(hares.triangd, "html") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                fixed_thead = TRUE) %>%
  scroll_box(width = "100%", height = "500px")

# kable(head(hares.triangd), "latex", booktabs = TRUE) %>% 
#   kable_styling(latex_options = "scale_down", position = "center")
```

### Visualizing the Triangulations
Below, we take a look at the "clean" relocation data set for each snowshoe hare.
In the following maps, the color of the dots corresponds to the day in which
the triangulation for that dot was carried out.

First, we calculate a 90% Minimum Convex Polygon around the complete dataset of
triangulated relocations from all individuals over the three years of this 
study. This will only be used to clip the FRI shapefile showing the various
forest polygons to the area of interest for this study, so as to speed up 
spatial mapping in R. 
```{r mapping-MCP, echo=TRUE, tidy=TRUE}
# finally, remove the two collars that are not in Bloomfield
hares.triangd <- subset(hares.triangd, hares.triangd$Frequency != "149.374" & 
                          hares.triangd$Frequency != "149.474")

# add a grid identifier to the hares.triangd dataframe to calculate an overall
# home range
hares.triangd$Grid <- "Bloomfield"

# calculate an overall home range using 100% MCP to define study area for 
# mapping purposes ONLY
bl.mcp.all <- mcp(hares.triangd[, 10], percent = 90)

# transform the 100% MCP into a simple features object
bl.mcp.all <- st_as_sf(bl.mcp.all)
```

Then, we apply a 600 m buffer around this 90% MCP, to represent the uncertainty 
around this estimate --- even if it is taking all relocations into account. In 
the image below, the blue polygon is the 90% MCP and the red one is the 500 m 
buffer around it.
```{r mapping-MCP-buffer,echo=TRUE,tidy=TRUE}
bl.mcp.all500 <- st_buffer(bl.mcp.all, dist = 600) 

tm_shape(bl.mcp.all500) + tm_polygons(alpha = 0, border.col = "red") + 
  tm_shape(bl.mcp.all) + tm_polygons(alpha = 0, border.col = "blue") + 
  tm_compass() + tm_scale_bar()
```

Now, we use the function `st_intersect()` in the `sf` package to perform the 
actual clipping of the FRI shapefile to the buffer we just drew.
```{r bg-layer-loading, echo=TRUE, tidy=TRUE, fig.cap="Map of the study area and the forest stands within 600 meters of the 90% MCP estimate border calculated from the whole triangulation dataset. Red dots represent trap locations on the live-trapping grid. "}

# and set it to the same crs as the triangulation data
forest_reproj <- st_transform(forest_gdb, crs = st_crs(hares.triangd))

# load grid trap locations shapefile
bl_grid_pts <- st_transform(bl_grid_points, crs = st_crs(hares.triangd))

# now, let's intersect the forest
forest_clip <- st_intersection(bl.mcp.all500, forest_reproj)
rm(forest_gdb) # remove the large FRI shapefile to make spatial R go faster
rm(forest_reproj) # remove the large FRI shapefile to make spatial R go faster

tm_shape(forest_clip) + tm_polygons(alpha = 0) + tm_shape(bl_grid_pts) + 
  tm_dots(size = 0.3, shape = 18, col = "red") + tm_compass() + tm_scale_bar()
```

Finally, we split the `Date` variable in the `hares.triangd` dataset into its 
three components: `Year`, `Month`, `Day`. This will be helpful for plotting
later on. Note that we do not remove the `Date` variable from the new dataset,
as it will also be helpful at later stages.

```{r triang-date-conversion, echo=TRUE, tidy=TRUE}
# First, we are going to convert hares.triangd into a normal dataframe 
# and then split the field "Date" into its component elements "Year", "Month", 
# and "Day"
hares.triangd.longdate <- as.data.frame(hares.triangd) %>% 
  separate(., "Date", into = c("Year", "Month", "Day"), remove = FALSE) 

# Now, to turn the hares.traingd.LG dataframe back into a SpatialPointsDataFrame
# First, separate the columns containing the X and Y (Lat and Long) coordinates
# from the rest of the dataframe
hares.triangd.LG.coords <- hares.triangd.longdate[, c(14,15)]

# Use SpatialPointsDataFrame() to turn hares.triangd.LG into a spatial object
hares.triangd.LG <- SpatialPointsDataFrame(coords = hares.triangd.LG.coords, 
                                           data = hares.triangd.longdate, 
                                           proj4string = crs(hares.triangd))
```

Once we have this map of the area around out sampling grid, which shows the grid 
itself in red, we can plot the points triangulated earlier for each hare and 
see where they are located in space. As well, we can colour the points based on
date of collection and identify hares by eartag and year of sampling, which allows
for identification of those individuals with more than one sampling season.

```{r reloc-mapping, echo=TRUE, tidy=TRUE, warning=FALSE, message=FALSE, fig.cap="Maps of the triangulated relocations for each hare for each year of sampling."}
# create list of names of collars from Bloomfield study site for mapping each
# BL set of points and future mapping/listing use
UniqCIDs <- UniqIDs[UniqIDs!="149.374"]
UniqCIDs <- UniqCIDs[UniqCIDs!="149.474"]

# allocate empty list to store tmap objects for each map created below
hares.dots.maps <- vector("list", length(UniqCIDs))

# create a palette for colouring the dots
triangdpalette <- viridisLite::viridis(20, begin = 0.5, end = 1)

# loop to produce a map for each hare showing a heatmap of the kUD and the 
# 50% and 90% isopleths
for (i in 1:length(UniqCIDs)) {
    # store contours for the current collar in a temporary linear object
    dots.temp <- subset(hares.triangd.LG, hares.triangd.LG$Frequency == UniqCIDs[i]) 
    
    # add a crs to the temporary linear object
    crs(dots.temp) <- crs(forest_clip) 
    
    bbox_temp <- st_bbox(dots.temp) # current bounding box
    
    xrange <- bbox_temp$xmax - bbox_temp$xmin # range of x values
    yrange <- bbox_temp$ymax - bbox_temp$ymin # range of y values
    
    bbox_temp[1] <- bbox_temp[1] - (0.05 * xrange) # xmin - left
    bbox_temp[3] <- bbox_temp[3] + (0.05 * xrange) # xmax - right
    bbox_temp[2] <- bbox_temp[2] - (0.05 * yrange) # ymin - bottom
    bbox_temp[4] <- bbox_temp[4] + (0.05 * yrange) # ymax - top
    
    bbox_temp <- bbox_temp %>%  # take the bounding box ...
      st_as_sfc() # ... and make it a sf polygon
    
    # save the number of levels in Date for later plotting
    col.lvls <- nlevels(as.factor(dots.temp$Date)) 
    
    # create a map using tmap and store it in its own slots in the list created
    # above
    hares.dots.maps[[i]] <- tm_shape(forest_clip) + 
      tm_borders(col = "grey", lwd = 0.25) +
      tm_shape(bl_grid_pts) + tm_dots(size = 0.15, 
                                      shape = 18, col = "red", alpha = 0.75) +
      tm_shape(dots.temp, bbox = bbox_temp, is.master = TRUE) + 
      tm_dots(size = 0.2, shape = 21, col = "Date",
              palette = triangdpalette, 
              border.col = "black") + 
# tm_text("Date", auto.placement = TRUE, size = 0.75, xmod = 1, ymod = 0.5) +
      tm_compass(position = c("RIGHT", "BOTTOM"), text.size = 1, size = 1) +
      tm_scale_bar(position = c("RIGHT", "BOTTOM"), width = 0.3) + 
      tm_layout(main.title = paste("Ear Tag:", dots.temp$EarTag,"Year:", 
                                   dots.temp$Year),
                # title = paste("Ear tag:", dots.temp$EarTag),
                # title.position = c("LEFT", "TOP"),
                legend.show = FALSE,
                # legend.outside = TRUE, 
                # legend.outside.position = "right",
                # legend.text.size = 1.1,
                # legend.title.size = 1.5,
                main.title.size = 0.5,
                asp = 1,
                frame = TRUE) +
      tmap_options(max.categories = col.lvls)
    
    # Sys.sleep(1)
    # browser()
    
    # print(hares.dots.maps[[i]])
    
    rm(dots.temp) # remove the temp dots object to avoid errors
}


tmap_arrange(hares.dots.maps) # for plotting in an html document

# tmap_arrange(hares.dots.maps[1:18])
# tmap_arrange(hares.dots.maps[19:35]) # if running analyses including 2017 data
# tmap_arrange(hares.dots.maps[17:31]) # if running analyses without 2017 data
```

## Home Range Estimation
After preliminary analyses, we decided to use a kernel Utilization Distribution 
(kUD) approach to estimating home range size, following the recommendations of 
@Borger2006. 

kUD works by estimating a smoothing parameter, *h* [@Calenge2006]. There are two 
main methods available within the package `adehabitatHR` to estimate *h*: an 
*ad hoc* method, commonly referred to as *reference bandwidth*, and a method 
based on Least Square Cross-Validation (LSCV). After initially using the LSCV 
method, we eventually switched to the *ad hoc* ('href') one. This is because 
with LSCV there were issues of the algorithm estimating *h* not converging 
within the range of values imposed on h. 

As noted here: 

* https://animov.faunalia.narkive.com/NnG9nLKl/again-kernelud-and-lscv
* https://jamesepaterson.github.io/jamespatersonblog/04_trackingworkshop_kernels

this is an ongoing issue with LSCV-powered kUD estimation, one which 
statisticians are still trying to solve. Using the *ad hoc* or *reference* 
method is thus more conservative in this case, and likely more conducive to 
comparisons between our present work and other studies.

To estimate the kernel utilization distribution, we need to specify a `grid` 
argument in the `kernelUD()` function. We will use the grid of one of our 
StDM-predicted rasters for this (e.g., vaanCN). To keep things consistent, 
we are going to use the extent of the `forest_clip` object we created above to 
clip the StDM raster to our study area. Note that the grid is the same for 
all StDM-predicted rasters, so this step is raster-agnostic. 

We will then estimate the kUD for each hare and then extract home range area. 
We will focus on the kUD's "core area", here defined as the 
areas in the home range where the probability of relocating an individuals is 1 
in 2 (i.e., 50%; @Borger2006), as well as the area within the 75% and 90% 
isopleths.
```{r homerange-estimation, echo=TRUE, tidy=TRUE, warning=FALSE}
# before estimating the kUD, remove collars 149.513, 149.555, 150.032, 
# 150.052, 150.154 due to too few relocations available for estimating a 
# reliable kernel Utilization Distribution
hares.triangd <- subset(hares.triangd, hares.triangd$Frequency != "149.555" 
                        # & hares.triangd$Frequency != "150.032" & 
                        # hares.triangd$Frequency != "149.513" & 
                        # hares.triangd$Frequency != "150.052" & 
                        # hares.triangd$Frequency != "150.154"
)
 
# reproject the forest_clip object to the projection fo the StDM raster
# forest_clipTmerc<- st_transform(forest_clip, crs(vaanCN))

# convert clipped raster to SpatialPixelsDataFrame so it can be used in kUD 
# analysis 
vaancnASC <- asc.from.raster(vaancnclip)
vaanCNspdf <- asc2spixdf(vaancnASC)

# check the class of the new object
class(vaanCNspdf)

# reproject the triangulation data to the same projection of the StDM raster, 
# so we can actually map the points on top of the raster and kernelUD() can 
# work properly
hares.triangd<- spTransform(hares.triangd, crs(vaanCN))

hares.kUD <- kernelUD(hares.triangd[,8], h = 'href', grid = vaanCNspdf, 
                      extent = 1, same4all = FALSE)

# If reverting back to using LSCV to estimate h, double-check that minimization
# of the cross-validation criteria is successful using:
# plotLSCV(hares.kUD)

# Estimate kUD area using a range of percent levels
kUD.hr.estimates <- kernel.area(hares.kUD, percent = seq(50, 95, 5), 
                                unout = "ha")

# and extract values only for the core area to be used in later modelling
hrArea.50 <- kUD.hr.estimates[1, ]
hrArea.50 <- tidyr::pivot_longer(hrArea.50, cols = 1:ncol(hrArea.50), 
                                 names_to = "CollarFrequency", 
                                 values_to = "HRsize", names_prefix = "X")

# now, let's extract the home range contour
hares.kUDhr.50 <- getverticeshr(hares.kUD, percent = 50)

hares.kUDhr.75 <- getverticeshr(hares.kUD, percent = 75)

hares.kUDhr.90 <- getverticeshr(hares.kUD, percent = 90)
```

To check if the grid was large enough to estimate the kUD of each hare, we plot 
the home range contours just estimated on the raster itself. If the contours 
of the home range are completely inside the raster clip, then our estimation 
worked and we can move on.

```{r 50kUD-extent, echo=TRUE, tidy=TRUE, fig.cap="Home range size estimates, produced using the 50% kUD isopleth, and StDM predictions for Blueberry C:N. As all home ranges are contained within the clipped raster, using the raster's extent as grid parameter to estimate home range size in kernelUD() is a viable approach."}

# isolate the extent of reprojected our study area
extent_forest_clip <- extent(forest_clip)
vaancnclip_frst <- crop(vaancnclip, extent_forest_clip)

tm_shape(vaancnclip_frst) + tm_raster(style = "cont", palette = "-PRGn", 
                                 alpha = .5, title = "Blueberry C:N") + 
  tm_shape(hares.kUDhr.50) + 
  tm_borders() + 
  tm_compass() +
  tm_scale_bar() +
  tm_layout(legend.outside = TRUE,
            legend.outside.position = "right")
```

```{r 75kUD-extent, echo=TRUE, tidy=TRUE, fig.cap="Home range size estimates, produced using the 75% kUD isopleth, and StDM predictions for Blueberry C:N. As all home ranges are contained within the clipped raster, using the raster's extent as grid parameter to estimate home range size in kernelUD() is a viable approach."}
tm_shape(vaancnclip_frst) + tm_raster(style = "cont", palette = "-PRGn", 
                                 alpha = .5, title = "Blueberry C:N") + 
  tm_shape(hares.kUDhr.75) + 
  tm_borders(col = "blue") + 
  tm_compass() +
  tm_scale_bar() +
  tm_layout(legend.outside = TRUE,
            legend.outside.position = "right")
```

```{r 90kUD-extent, echo=TRUE, tidy=TRUE, fig.cap="Home range size estimates, produced using the 90% kUD isopleth, and StDM predictions for Blueberry C:N. As all home ranges are contained within the clipped raster, using the raster's extent as grid parameter to estimate home range size in kernelUD() is a viable approach."}
tm_shape(vaancnclip_frst) + tm_raster(style = "cont", palette = "-PRGn", 
                                 alpha = .5, title = "Blueberry C:N") + 
  tm_shape(hares.kUDhr.90) + 
  tm_borders(col="red") + 
  tm_compass() +
  tm_scale_bar() +
  tm_layout(legend.outside = TRUE,
            legend.outside.position = "right")

```

### kUD Rarefaction Analysis

This code chunk builds accumulation curves for the kernel Utilization 
Distribution estimates from our sample of 30 snowshoe hares. The code we use 
in this script was developed by Dr. Bernardo Brando Niebuhr [@movecology].

```{r kud-rarefaction-simple, echo=TRUE, tidy=TRUE, warning=FALSE}
# create a vector of unique IDs for the loop below
ids <- unique(hares.triangd$Frequency)

# store the 'working' percentage for which kUD isopleth to use
KDE_percentage <- 50

cumHRkde <- list() ## List with cumulative sample size for all individuals

for (i in 1:length(ids)) {  ## loop for individuals
  temp <- hares.triangd[which(hares.triangd$Frequency == ids[i]),] # select an individual
  # Here we do not use the points as they are, but randomize their order using 'sample'
  temp <- SpatialPoints(coordinates(temp)[sample(length(temp)),], CRS(proj4string(hares.triangd)))
  cumulative <- vector()
  for(k in 5:length(temp)){  ##loop for sample size from 5 locations to all locations
    UD <- kernelUD(temp[1:k,], h = 'href', grid = vaanCNspdf, 
                   extent = 1, same4all = FALSE)
    cumulative[k] <- kernel.area(UD, percent = KDE_percentage)
  }  
  cumulative <- cumulative[5:length(cumulative)]
  cumHRkde[[i]] <- data.frame(hr = unlist(cumulative), ssize = 5:length(temp))
}
 
names(cumHRkde) <- ids
# cumHRkde

# Plotting -- with ggplot, base graphics turned off
# par(mfrow = c(5, 7))
# # Seeing cummulative MCP area plots
# for(i in 1:length(ids)) { ## plot all curves with 2 seconds interval
#   plot(cumHRkde[[i]]$hr ~ cumHRkde[[i]]$ssize, cex=0.5, pch=16, main=ids[i],
#       xlab = "Number of locations", ylab = paste0("KDE ", KDE_percentage, "% area (ha)"))
#   points(cumHRkde[[i]]$hr ~ cumHRkde[[i]]$ssize, type="l", lwd=0.7, lty=2)
#   # Sys.sleep(1)
# }
# that looks strange! maybe we should use it with resampling.

acc_patchwork <- list()

for (i in 1:length(ids)) {
  temp_ggp <- fortify(cumHRkde[[i]])
  acc_patchwork[[i]] <- ggplot(temp_ggp, aes(x = ssize, y = hr), group = hr) + geom_point(size = 1, shape = 1) + geom_line(linetype = 2, size = 0.25) + xlab("Number of Locations") + ylab(paste0("KDE ", KDE_percentage, "% area (ha)")) + theme_classic()
}

# acc_patchwork

wrap_plots(acc_patchwork, ncol = 5)
```

Now, let's randomize the order in which the locations are added to the plot 
before estimating the kUD. 

> Note that this is a time-consuming step.

```{r kud-rarefaction-resampIter-1, echo=TRUE, tidy=TRUE, warning=FALSE}
# Now we're going to repeat that several times per individual. with resampling

iterations <- 20

### Parameters to control
KDE_percentage <- 50

cumHRkde <- list() ## List with cumulative sample size for all individuals

for (i in 1:length(ids)) {  ## loop for individuals
  # print(i)
  cumHRkde[[i]] <- list()
  for(j in 1:iterations) { # Loop for iterations
    temp <- hares.triangd[which(hares.triangd$Frequency == ids[i]),]
    # Here we do not use the points as they are, but randomize their order
    temp <- SpatialPoints(coordinates(temp)[sample(length(temp)),], CRS(proj4string(hares.triangd)))
    cumulative <- vector()
    for(k in 5:length(temp)){  ##loop for sample size from 5 locations to all locations
      UD <- kernelUD(temp[1:k,], h = 'href', grid = vaanCNspdf, 
                     extent = 1, same4all = FALSE)
      cumulative[k] <- kernel.area(UD, percent = KDE_percentage)
    }  
    cumulative <- cumulative[5:length(cumulative)]
    cumHRkde[[i]][[j]] <- data.frame(hr = unlist(cumulative), ssize = 5:length(temp))
  }
}

names(cumHRkde) <- ids
# cumHRkde

# Plotting
# par(mfrow=c(5,7))

# Seeing cummulative kde area plots -- with ggplot, basic graphics turned off
# for(i in 1:length(ids)) { ## plot all curves with 2 seconds interval
#   cum.df <- as.data.frame(cumHRkde[[i]])
#   cumHR.df.mult <- cum.df[,c(2,seq(1, ncol(cum.df), 2))]
#   
#   cumHR.df.mult2 <- data.frame(npoins = rep(cumHR.df.mult[,1], iterations), HR = unlist(c(cumHR.df.mult[,2:ncol(cumHR.df.mult)])))
#   plot(cumHR.df.mult2$npoins, cumHR.df.mult2$HR, cex=0.5, pch=16, col = 'grey', main=ids[i],
#        xlab = "Number of locations",ylab = paste0("KDE ", KDE_percentage, "% area (ha)"))
#   points(unique(cumHR.df.mult2$npoins), apply(cumHR.df.mult[,2:ncol(cumHR.df.mult)], MARGIN = 1, FUN = median), type="l", lwd=3, lty=1)
#   
#   # Sys.sleep(1)
# }

iter_patchwork <- list()

for (i in 1:length(ids)) {
  cum.df <- as.data.frame(cumHRkde[[i]])
  cumHR.df.mult <- cum.df[,c(2,seq(1, ncol(cum.df), 2))]
  
  cumHR.df.mult2 <- data.frame(npoins = rep(cumHR.df.mult[,1], iterations), HR = unlist(c(cumHR.df.mult[,2:ncol(cumHR.df.mult)])))
  iter_patchwork[[i]] <- ggplot(cumHR.df.mult2, aes(x = npoins, y = HR)) + 
    geom_point(size = 0.5, col = "grey", alpha = 0.5) + 
    stat_summary(fun = median, fun.min = median, fun.max = median,
                 geom = "line", size = 0.5) +
    ylab(paste0(KDE_percentage, "% area (ha)")) +
    xlab("Number of locations") +
    ggtitle(ids[i]) +
    theme_classic()
}

# iter_patchwork

wrap_plots(iter_patchwork, ncol = 5) + plot_annotation(title = paste0("KDE ", KDE_percentage, "% area (ha)"))
```

An alternative way of visualizing these accumulation curves, using boxplots.

```{r kud-rarefaction-resampIter-2, echo=TRUE, tidy=TRUE, warning=FALSE}
# Another way of plotting, with boxplot -- with ggplot, basic graphics turned off
# par(mfrow=c(5,7))
# # Seeing cummulative kde area plots
# for(i in 1:length(ids)) { ## plot all curves with 2 seconds interval
#   cum.df <- as.data.frame(cumHRkde[[i]])
#   cumHR.df.mult <- cum.df[,c(2,seq(1, ncol(cum.df), 2))]
#   
#   cumHR.df.mult2 <- data.frame(npoins = rep(cumHR.df.mult[,1], iterations), HR = unlist(c(cumHR.df.mult[,2:ncol(cumHR.df.mult)])))
#   boxplot(cumHR.df.mult2$HR ~ as.factor(cumHR.df.mult2$npoins), 
#           main=ids[i], xlab = "Number of locations",
#           ylab = paste0("KDE ", KDE_percentage, "% area (ha)"))
#   # Sys.sleep(1)
# }

bp_patchwork <- list()

for (i in 1:length(ids)) {
  cum.df <- as.data.frame(cumHRkde[[i]])
  cumHR.df.mult <- cum.df[,c(2,seq(1, ncol(cum.df), 2))]
  
  cumHR.df.mult2 <- data.frame(npoins = rep(cumHR.df.mult[,1], iterations), HR = unlist(c(cumHR.df.mult[,2:ncol(cumHR.df.mult)])))
  bp_patchwork[[i]] <- ggplot(cumHR.df.mult2, aes(x = npoins, y = HR)) + 
    geom_boxplot(aes(group = npoins), size = 0.5, fill = "grey", alpha = 0.3) + 
    ylab(paste0(KDE_percentage, "% area (ha)")) +
    xlab("Number of locations") +
    ggtitle(ids[i]) +
    theme_classic()
}

# bp_patchwork

wrap_plots(bp_patchwork, ncol = 5) + plot_annotation(title = paste0("KDE ", KDE_percentage, "% area (ha)"))

```

### Overlap in kUD

The code below allows us to assess the overlap in the kUD of the four
individuals with 2 years each of telemetry sampling. We follow the advice
provided by [@Fieberg2006], and calculate several measures of overlap to then
compare them. In particular, we use percent overlap (`HR`), the probability of
overlap (`PHR`), the utilization distribution overlap index (`UDOI`), and the
Bhattacharyya's affinity index (`BA`). Here we briefly describe each of these
indexes, referring the interested reader to @Fieberg2006 for an in-depth 
discussion of their strengths and pitfalls. 

`HR` captures the proportion of home range estimate of one animal that is 
overlapped by that of another animal --- in our case, by the same animal the 
following year. `PHR` calculates the probability of an animal to be found in the 
home range of another animal. `UDOI` calculated the overlap between the two 
utilization distribution of the animal estimated from the two consecutive years.
Finally, `BA` capture the affinity between the two samples of points collected
in consecutive years from the same individual. 

In the SI, we include a shorter version of resulting table, reporting only the 
overlap measured through PHR and UDOI (Table S4).
```{r kUD-overlap, echo=TRUE, tidy=TRUE, message=FALSE, warning=FALSE}
hares.triangd.LG <- spTransform(hares.triangd.LG, crs(vaanCN))

# now, subset the dataset into four dfs each contianing only the one individual
A1425.points <- subset(hares.triangd.LG, hares.triangd.LG$EarTag == "A1425")
A1698.points <- subset(hares.triangd.LG, hares.triangd.LG$EarTag == "A1698")
A3719.points <- subset(hares.triangd.LG, hares.triangd.LG$EarTag == "A3719")
A3769.points <- subset(hares.triangd.LG, hares.triangd.LG$EarTag == "A3769")


# calculate kUD just for these points, as the kerneloverlap() function cannot
# take the argument "grid"
A1425.kUD <- kernelUD(A1425.points[,7], grid = vaanCNspdf, h = "href", extent = 1, same4all=F)
A1698.kUD <- kernelUD(A1698.points[,7], grid = vaanCNspdf, h = "href", extent = 1, same4all=F)
A3719.kUD <- kernelUD(A3719.points[,7], grid = vaanCNspdf, h = "href", extent = 1, same4all=F)
A3769.kUD <- kernelUD(A3769.points[,7], grid = vaanCNspdf, h = "href", extent = 1, same4all=F)

# and calcualte overlap using a variety of methods, 
# following Fienberg & Kochanny 2006
# Method "HR"
A1425.HR <- kerneloverlaphr(A1425.kUD, method = "HR", percent = 50) %>% as_tibble() %>% 
  add_column(., EarTag = "A1425", Method = "HR", .before = 1)
A1698.HR <- kerneloverlaphr(A1698.kUD, method = "HR", percent = 50) %>% as_tibble() %>% 
  add_column(., EarTag = "A1698", Method = "HR", .before = 1)
A3719.HR <- kerneloverlaphr(A3719.kUD, method = "HR", percent = 50) %>% as_tibble() %>% 
  add_column(., EarTag = "A3719", Method = "HR", .before = 1)
A3769.HR <- kerneloverlaphr(A3769.kUD, method = "HR", percent = 50) %>% as_tibble() %>% 
  add_column(., EarTag = "A3769", Method = "HR", .before = 1)

# Method "PHR"
A1425.PHR <- kerneloverlaphr(A1425.kUD, method = "PHR", percent = 50) %>% as_tibble() %>% 
  add_column(., EarTag = "A1425", Method = "PHR", .before = 1)
A1698.PHR <- kerneloverlaphr(A1698.kUD, method = "PHR", percent = 50) %>% as_tibble() %>% 
  add_column(., EarTag = "A1698", Method = "PHR", .before = 1)
A3719.PHR <- kerneloverlaphr(A3719.kUD, method = "PHR", percent = 50) %>% as_tibble() %>% 
  add_column(., EarTag = "A3719", Method = "PHR", .before = 1)
A3769.PHR <- kerneloverlaphr(A3769.kUD, method = "PHR", percent = 50) %>% as_tibble() %>% 
  add_column(., EarTag = "A3769", Method = "PHR", .before = 1)

# Method "UDOI"
A1425.UDOI <- kerneloverlaphr(A1425.kUD, method = "UDOI", percent = 50) %>% as_tibble() %>% 
  add_column(., EarTag = "A1425", Method = "UDOI", .before = 1)
A1698.UDOI <- kerneloverlaphr(A1698.kUD, method = "UDOI", percent = 50) %>% as_tibble() %>% 
  add_column(., EarTag = "A1698", Method = "UDOI", .before = 1)
A3719.UDOI <- kerneloverlaphr(A3719.kUD, method = "UDOI", percent = 50) %>% as_tibble() %>% 
  add_column(., EarTag = "A3719", Method = "UDOI", .before = 1)
A3769.UDOI <- kerneloverlaphr(A3769.kUD, method = "UDOI", percent = 50) %>% as_tibble() %>% 
  add_column(., EarTag = "A3769", Method = "UDOI", .before = 1)

# Method "BA"
A1425.BA <- kerneloverlaphr(A1425.kUD, method = "BA", percent = 50) %>% as_tibble() %>% 
  add_column(., EarTag = "A1425", Method = "BA", .before = 1)
A1698.BA <- kerneloverlaphr(A1698.kUD, method = "BA", percent = 50) %>% as_tibble() %>% 
  add_column(., EarTag = "A1698", Method = "BA", .before = 1)
A3719.BA <- kerneloverlaphr(A3719.kUD, method = "BA", percent = 50) %>% as_tibble() %>% 
  add_column(., EarTag = "A3719", Method = "BA", .before = 1)
A3769.BA <- kerneloverlaphr(A3769.kUD, method = "BA", percent = 50) %>% as_tibble() %>% 
  add_column(., EarTag = "A3769", Method = "BA", .before = 1)

# Visualization
# join all together and then export as latex table

# PHR and UDOI (Preferred)
olap <- full_join(A1425.PHR, A1425.UDOI) %>% full_join(., A1425.BA) %>% 
  full_join(., A1425.HR) %>% full_join(., A1698.PHR) %>% 
  full_join(., A1698.UDOI) %>% full_join(., A1698.BA) %>% 
  full_join(., A1698.HR) %>% full_join(., A3719.PHR) %>%
  full_join(., A3719.UDOI) %>%full_join(., A3719.BA) %>% 
  full_join(., A3719.HR) %>% full_join(., A3769.PHR) %>% 
  full_join(., A3769.UDOI) %>% full_join(., A3769.BA) %>% 
  full_join(., A3769.HR) 

olap <- olap[, c(1, 2, 5, 3, 4)]

kable(olap[, 2:5]) %>%  group_rows(index = setNames(rle(olap$EarTag)[[1]], rle(olap$EarTag)[[2]])) %>%
   kable_styling(bootstrap_options = c("hover", "condensed"), 
                 fixed_thead = TRUE) %>% 
   scroll_box(width = "100%", height = "100%")

# just PHR and UDOI for the SI
# olap_summary <- full_join(A1425.PHR, A1425.UDOI) %>% full_join(., A1698.PHR) %>%
#   full_join(., A1698.UDOI) %>% full_join(., A3719.PHR) %>%
#   full_join(., A3719.UDOI) %>% full_join(., A3769.PHR) %>%
#   full_join(., A3769.UDOI) %>% as_tibble() %>% select(EarTag, Method, "2017", "2018", "2019") %>% gt() %>% cols_label(EarTag = "Ear Tag") %>% gtsave(., filename = "../Results/OverlapTab.tex")
```

## Stoichiometric data extraction within home ranges
Now that we have estimates of the kUD and home range size for our snowshoe 
hares, we can use these spatial objects to extract the values of C:N:P 
stoichiometric ratios from Stoichiometric Distribution Models (StDMs; *sensu*
@Leroux2017) from within the core areas of the home ranges of our snowshoe 
hares. 

Note that this step involves only the use of the 50% kUD area estimates we 
calculated in the code chunk above and of the StDM-generated rasters predicting 
values of the C:N, C:P, and N:P ratios over the whole ecoregion. No clipping 
took place beforehand using the 90% Minimum Convex Polygon produced earlier 
(i.e., object `bl.mcp.all`), as that is only used in mapping instances to reduce
the time needed to produce maps through R.

```{r stoichHR, echo=TRUE, message=FALSE, tidy=TRUE}
# allocate an empty list of projected kUD as Spatial objects
stoich.hares <- data.frame(CollarFrequency = character(0),
                      # ABBA_CN = numeric(0), ABBA_CP = numeric(0), 
                      ACRU_CN = numeric(0), ACRU_NP = numeric(0), 
                      VAAN_CN = numeric(0), VAAN_CP = numeric(0), 
                      VAAN_NP = numeric(0))

# remove collars with too few relocations for further analysis
hares.kUDhr.50$id <- as.factor(hares.kUDhr.50$id)

hares.kUDhr.50 <- subset(hares.kUDhr.50, 
                         hares.kUDhr.50$id != "149.555" # &
                         # hares.kUDhr.50$id != "150.032" &
                         # hares.kUDhr.50$id != "150.052" &
                         # hares.kUDhr.50$id != "150.154" 
                         )

hares.kUDhr.50$id <- droplevels(hares.kUDhr.50$id)

# Create new indexing vector only for Bloomfield
UniqCIDs.bl <- as.vector(droplevels(unique(hares.kUDhr.50$id)))

for (i in 1:length(UniqCIDs.bl)) {
  # select the home range of interest
  temp.hr.50 <- hares.kUDhr.50[hares.kUDhr.50$id == UniqCIDs.bl[i],]
  
  # transform CRS of temp.kUDhr to that of stoich_stack - NOT needed anymore 
  # after May 1, 2020, edits
  # temp.hr.50 <- spTransform(temp.hr.50, CRSobj = crs(stoich_stack))  
  
  # extract the stoich values for the home range of interest and store it 
  # temporarily in a dataframe
  temp.stoich.hr50 <- raster::extract(stoich_stack, 
                                      temp.hr.50, method = "simple", 
                                      weights = TRUE, df = TRUE, 
                                      small = TRUE,
                                      normalizeWeights = TRUE, 
                                      cellnumbers = TRUE)
  # browser()
  
  # assign home range id to the temporary stoich dataframe
  temp.stoich.hr50$CollarFrequency <- UniqCIDs.bl[i]
  
  # bind the temporary stoich dataframe with the permanent one
  
  stoich.hares <- dplyr::bind_rows(stoich.hares, temp.stoich.hr50)
  
  # projkUDsp.df <- na.omit(projkUDsp.df)
  
  # browser()
  
  # print(i)
}

kable(stoich.hares, "html") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                fixed_thead = TRUE) %>%
  scroll_box(width = "100%", height = "500px")

# kable(head(stoich.hares), "latex", booktabs = TRUE) %>% 
#   kable_styling(latex_options = "scale_down", position = "center")
```


### Stoichiometric variation within home ranges {.tabset .tabset-fade .tabset-pills}
Below, we show boxplots for the C:N, C:P, and N:P ratios for red maple 
(*Acer rubrum*) and blueberry (*Vaccinium angustifolium*) as found within the 
core areas of the home ranges of `r unique(hares.triangd$EarTag)` snowshoe hares 
living on the Bloomfield grid during the summer months of 2017, 2018, and 2019.

#### C:N
```{r acruCN-boxplot, echo=TRUE, tidy=TRUE, fig.cap="Variability in stoichiometric values for Red Maple C:N in snowshoe hares home ranges."}
ggplot(data = stoich.hares, aes(x = CollarFrequency, 
                                y = ACRU_CN,
                                col = CollarFrequency)) + 
  geom_boxjitter(na.rm=TRUE, outlier.intersect = TRUE,
                 jitter.alpha = 0.5, outlier.alpha = 0.5) + 
  scale_color_viridis_d() +
  xlab("Collar Frequency") + 
  ylab("C:N Ratio") +
  ggtitle("Red Maple C:N") +
  coord_flip() +
  theme_clean() +
  theme(legend.position = "none",
        axis.line = element_line(colour = "black"),
        aspect.ratio = 1)
```

```{r vaanCN-boxplot, echo=TRUE, tidy=TRUE, fig.cap="Variability in stoichiometric values for Blueberry C:N in snowshoe hares home ranges."}
ggplot(data = stoich.hares, aes(x = CollarFrequency, 
                               y = VAAN_CN,
                               col = CollarFrequency)) + 
  geom_boxjitter(na.rm=TRUE, outlier.intersect = TRUE,
                 jitter.alpha = 0.5, outlier.alpha = 0.5) + 
  scale_color_viridis_d() +
  xlab("Collar Frequency") + 
  ylab("C:N Ratio") +
  ggtitle("Blueberry C:N") +
  coord_flip() +
  theme_clean() +
  theme(legend.position = "none",
        axis.line = element_line(colour = "black"),
        aspect.ratio = 1)

# ggsave(filename = "../Results/vaanStoich_bplot.png", 
#        width = 18, height = 10, dpi = 300, device = "png")
```

#### C:P
```{r vaanCP-boxplot, echo=TRUE, tidy=TRUE, fig.cap="Variability in stoichiometric values for Blueberry C:P in snowshoe hares home ranges."}
ggplot(data = stoich.hares, aes(x = CollarFrequency, 
                                y = VAAN_CP,
                                col = CollarFrequency)) + 
  geom_boxjitter(na.rm=TRUE, outlier.intersect = TRUE,
                 jitter.alpha = 0.5, outlier.alpha = 0.5) + 
  scale_color_viridis_d() +
  xlab("Collar Frequency") + 
  ylab("C:P Ratio") +
  ggtitle("Blueberry C:P") +
  coord_flip() +
  theme_clean() +
  theme(legend.position = "none",
        axis.line = element_line(colour = "black"),
        aspect.ratio = 1)
```

#### N:P
```{r acruNP-boxplot, echo=TRUE, tidy=TRUE, fig.cap="Variability in stoichiometric values for Red Maple N:P in snowshoe hares home ranges."}
ggplot(data = stoich.hares, aes(x = CollarFrequency, 
                                y = ACRU_NP,
                                col = CollarFrequency)) + 
  geom_boxjitter(na.rm=TRUE, outlier.intersect = TRUE,
                 jitter.alpha = 0.5, outlier.alpha = 0.5) + 
  scale_color_viridis_d() + 
  xlab("Collar Frequency") + 
  ylab("N:P Ratio") +
  ggtitle("Red Maple N:P") +
  coord_flip() +
  theme_clean() +
  theme(legend.position = "none",
        axis.line = element_line(colour = "black"),
        aspect.ratio = 1)
```

```{r vaanNP-boxplot, echo=TRUE, tidy=TRUE, fig.cap="Variability in stoichiometric values for Blueberry N:P in snowshoe hares home ranges."}
ggplot(data = stoich.hares, aes(x = CollarFrequency, 
                                y = VAAN_NP,
                                col = CollarFrequency)) + 
  geom_boxjitter(na.rm=TRUE, outlier.intersect = TRUE,
                 jitter.alpha = 0.5, outlier.alpha = 0.5) + 
  scale_color_viridis_d() +
  xlab("Collar Frequency") + 
  ylab("N:P Ratio") +
  ggtitle("Blueberry N:P") +
  coord_flip() +
  theme_clean() +
  theme(legend.position = "none",
        axis.line = element_line(colour = "black"),
        aspect.ratio = 1)
```
